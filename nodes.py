"""
ComfyUI æ˜¾å­˜ç®¡ç†èŠ‚ç‚¹ - æ”¹è¿›ç‰ˆ
æ”¯æŒæ™ºèƒ½æ˜¾å­˜é¢„ç•™ã€è‡ªåŠ¨æ¸…ç†ã€å¤šGPUç®¡ç†
"""

from typing import Any, Tuple, Optional, Union, Dict
import logging
import threading
import atexit
import gc
import torch
from comfy import model_management

# ============================================================================
# æ—¥å¿—é…ç½®
# ============================================================================

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("GPUMemoryManager")

# ============================================================================
# å¸¸é‡å®šä¹‰
# ============================================================================

GB_TO_BYTES = 1024 * 1024 * 1024
MB_TO_BYTES = 1024 * 1024
MIN_RESERVED_GB = 0.6
DEFAULT_RESERVED_GB = 1.0
MIN_SAFE_RESERVE_GB = 2.0
MAX_RESERVED_RATIO = 0.9  # æœ€å¤§é¢„ç•™æ¯”ä¾‹

# æ˜¾å­˜ä½¿ç”¨ç­–ç•¥
MEMORY_STRATEGY = {
    "tight": 0.8,    # ç´§å¼ ï¼šå¯ç”¨<20%
    "medium": 0.85,  # ä¸­ç­‰ï¼šå¯ç”¨20-40%
    "loose": 0.9     # å……è¶³ï¼šå¯ç”¨>40%
}

# ============================================================================
# GPUç®¡ç†ç±» (çº¿ç¨‹å®‰å…¨å•ä¾‹)
# ============================================================================

class GPUManager:
    """GPUç®¡ç†ç±»ï¼Œå°è£…pynvmlç›¸å…³æ“ä½œ - çº¿ç¨‹å®‰å…¨å•ä¾‹"""
    
    _instance = None
    _lock = threading.Lock()
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(GPUManager, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        # é¿å…é‡å¤åˆå§‹åŒ–
        if not GPUManager._initialized:
            with GPUManager._lock:
                if not GPUManager._initialized:
                    self.pynvml = None
                    self._pynvml_available = False
                    self._initialize_pynvml()
                    GPUManager._initialized = True
    
    def _initialize_pynvml(self) -> None:
        """åˆå§‹åŒ–pynvml"""
        try:
            import pynvml
            self.pynvml = pynvml
            pynvml.nvmlInit()
            self._pynvml_available = True
            logger.info("GPUç›‘æ§å·²åˆå§‹åŒ– (pynvml)")
        except ImportError:
            self._pynvml_available = False
            logger.warning("æœªå®‰è£…pynvmlåº“ï¼ŒGPUç›‘æ§åŠŸèƒ½ä¸å¯ç”¨ã€‚å®‰è£…å‘½ä»¤: pip install pynvml")
        except Exception as e:
            self._pynvml_available = False
            logger.error(f"åˆå§‹åŒ–pynvmlå¤±è´¥: {e}")
    
    def is_available(self) -> bool:
        """æ£€æŸ¥GPUç›‘æ§æ˜¯å¦å¯ç”¨"""
        return self._pynvml_available
    
    def get_gpu_count(self) -> int:
        """è·å–GPUæ•°é‡"""
        if not self._pynvml_available:
            return torch.cuda.device_count() if torch.cuda.is_available() else 0
        
        try:
            return self.pynvml.nvmlDeviceGetCount()
        except Exception as e:
            logger.error(f"è·å–GPUæ•°é‡å¤±è´¥: {e}")
            return 0
    
    def validate_gpu_index(self, gpu_index: int) -> bool:
        """éªŒè¯GPUç´¢å¼•æ˜¯å¦æœ‰æ•ˆ"""
        gpu_count = self.get_gpu_count()
        if gpu_index < 0 or gpu_index >= gpu_count:
            logger.warning(f"æ— æ•ˆçš„GPUç´¢å¼•: {gpu_index}ï¼Œå¯ç”¨èŒƒå›´: 0-{gpu_count-1}")
            return False
        return True
    
    def get_gpu_memory_info(self, gpu_index: int = 0) -> Optional[Tuple[float, float, float]]:
        """è·å–GPUæ˜¾å­˜ä¿¡æ¯
        
        Args:
            gpu_index: GPUç´¢å¼•ï¼Œé»˜è®¤ä¸º0
            
        Returns:
            (æ€»æ˜¾å­˜GB, å·²ç”¨æ˜¾å­˜GB, å¯ç”¨æ˜¾å­˜GB) æˆ– None
        """
        if not self._pynvml_available:
            return None
        
        if not self.validate_gpu_index(gpu_index):
            return None
            
        try:
            handle = self.pynvml.nvmlDeviceGetHandleByIndex(gpu_index)
            memory_info = self.pynvml.nvmlDeviceGetMemoryInfo(handle)
            
            total_gb = memory_info.total / GB_TO_BYTES
            used_gb = memory_info.used / GB_TO_BYTES
            free_gb = memory_info.free / GB_TO_BYTES
            
            return total_gb, used_gb, free_gb
        except Exception as e:
            logger.error(f"è·å–GPU{gpu_index}ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None
    
    def get_gpu_name(self, gpu_index: int = 0) -> Optional[str]:
        """è·å–GPUåç§°"""
        if not self._pynvml_available:
            return None
        
        try:
            handle = self.pynvml.nvmlDeviceGetHandleByIndex(gpu_index)
            name = self.pynvml.nvmlDeviceGetName(handle)
            return name.decode('utf-8') if isinstance(name, bytes) else name
        except Exception as e:
            logger.error(f"è·å–GPU{gpu_index}åç§°å¤±è´¥: {e}")
            return None
    
    def get_gpu_temperature(self, gpu_index: int = 0) -> Optional[int]:
        """è·å–GPUæ¸©åº¦"""
        if not self._pynvml_available:
            return None
        
        try:
            handle = self.pynvml.nvmlDeviceGetHandleByIndex(gpu_index)
            temp = self.pynvml.nvmlDeviceGetTemperature(
                handle, 
                self.pynvml.NVML_TEMPERATURE_GPU
            )
            return temp
        except Exception as e:
            logger.debug(f"è·å–GPU{gpu_index}æ¸©åº¦å¤±è´¥: {e}")
            return None
    
    def get_gpu_utilization(self, gpu_index: int = 0) -> Optional[int]:
        """è·å–GPUåˆ©ç”¨ç‡"""
        if not self._pynvml_available:
            return None
        
        try:
            handle = self.pynvml.nvmlDeviceGetHandleByIndex(gpu_index)
            utilization = self.pynvml.nvmlDeviceGetUtilizationRates(handle)
            return utilization.gpu
        except Exception as e:
            logger.debug(f"è·å–GPU{gpu_index}åˆ©ç”¨ç‡å¤±è´¥: {e}")
            return None
    
    def get_detailed_info(self, gpu_index: int = 0) -> Dict[str, Any]:
        """è·å–GPUè¯¦ç»†ä¿¡æ¯"""
        info = {
            "index": gpu_index,
            "available": False,
            "name": None,
            "memory": None,
            "temperature": None,
            "utilization": None
        }
        
        if not self.validate_gpu_index(gpu_index):
            return info
        
        info["available"] = True
        info["name"] = self.get_gpu_name(gpu_index)
        info["memory"] = self.get_gpu_memory_info(gpu_index)
        info["temperature"] = self.get_gpu_temperature(gpu_index)
        info["utilization"] = self.get_gpu_utilization(gpu_index)
        
        return info
    
    def cleanup(self) -> None:
        """æ¸…ç†èµ„æº"""
        if self._pynvml_available and self.pynvml:
            try:
                self.pynvml.nvmlShutdown()
                logger.info("GPUç›‘æ§å·²å…³é—­")
            except Exception as e:
                logger.error(f"å…³é—­GPUç›‘æ§æ—¶å‡ºé”™: {e}")

# ============================================================================
# å†…å­˜æ¸…ç†ç±»
# ============================================================================

class MemoryCleaner:
    """å†…å­˜æ¸…ç†å™¨ - çº¿ç¨‹å®‰å…¨"""
    
    _lock = threading.Lock()
    
    @staticmethod
    def clear_gpu_memory(gpu_index: int = 0) -> Dict[str, Any]:
        """æ¸…ç†GPUå†…å­˜ - çº¿ç¨‹å®‰å…¨
        
        Args:
            gpu_index: GPUç´¢å¼•
            
        Returns:
            æ¸…ç†ç»“æœä¿¡æ¯
        """
        with MemoryCleaner._lock:
            result = {
                "success": False,
                "torch_cuda": False,
                "gc_collected": 0,
                "before_memory": None,
                "after_memory": None,
                "freed_memory_gb": 0.0,
                "freed_memory_mb": 0.0
            }
            
            gpu_manager = GPUManager()
            
            # è·å–æ¸…ç†å‰å†…å­˜
            result["before_memory"] = gpu_manager.get_gpu_memory_info(gpu_index)
            
            try:
                # æ¸…ç†PyTorch CUDAç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    result["torch_cuda"] = True
                    logger.info("PyTorch CUDAç¼“å­˜å·²æ¸…ç†")
                
                # æ‰§è¡Œåƒåœ¾å›æ”¶
                collected = gc.collect()
                result["gc_collected"] = collected
                logger.info(f"åƒåœ¾å›æ”¶å®Œæˆï¼Œå›æ”¶å¯¹è±¡æ•°: {collected}")
                
                # è·å–æ¸…ç†åå†…å­˜
                result["after_memory"] = gpu_manager.get_gpu_memory_info(gpu_index)
                
                # è®¡ç®—é‡Šæ”¾çš„å†…å­˜
                if result["before_memory"] and result["after_memory"]:
                    _, used_before, _ = result["before_memory"]
                    _, used_after, _ = result["after_memory"]
                    freed_gb = used_before - used_after
                    freed_mb = freed_gb * 1024
                    
                    result["freed_memory_gb"] = freed_gb
                    result["freed_memory_mb"] = freed_mb
                    result["success"] = True
                    
                    if freed_gb > 0:
                        logger.info(f"æ˜¾å­˜æ¸…ç†æˆåŠŸ: é‡Šæ”¾äº† {freed_gb:.2f}GB ({freed_mb:.0f}MB)")
                    else:
                        logger.info("æ˜¾å­˜æ¸…ç†å®Œæˆï¼Œæœªé‡Šæ”¾é¢å¤–æ˜¾å­˜")
                else:
                    result["success"] = True
                    logger.info("æ˜¾å­˜æ¸…ç†å®Œæˆ")
                    
            except Exception as e:
                logger.error(f"æ¸…ç†æ˜¾å­˜æ—¶å‡ºé”™: {e}")
                result["success"] = False
            
            return result
    
    @staticmethod
    def clear_all_caches() -> None:
        """æ¸…ç†æ‰€æœ‰å¯èƒ½çš„ç¼“å­˜"""
        try:
            # PyTorchç¼“å­˜
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    with torch.cuda.device(i):
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
            
            # ComfyUIç¼“å­˜
            if hasattr(model_management, 'cleanup_models'):
                model_management.cleanup_models()
            
            # Pythonåƒåœ¾å›æ”¶
            gc.collect()
            
            logger.info("æ‰€æœ‰ç¼“å­˜å·²æ¸…ç†")
        except Exception as e:
            logger.error(f"æ¸…ç†æ‰€æœ‰ç¼“å­˜æ—¶å‡ºé”™: {e}")

# ============================================================================
# é€šç”¨ç±»å‹ä»£ç†
# ============================================================================

class AlwaysEqualProxy(str):
    """å§‹ç»ˆç›¸ç­‰çš„ä»£ç†å­—ç¬¦ä¸²ï¼Œç”¨äºé€šç”¨è¾“å…¥ç±»å‹"""
    
    def __eq__(self, _) -> bool:
        return True

    def __ne__(self, _) -> bool:
        return False

# ============================================================================
# æ˜¾å­˜ç­–ç•¥è®¡ç®—å™¨
# ============================================================================

class MemoryStrategyCalculator:
    """æ˜¾å­˜é¢„ç•™ç­–ç•¥è®¡ç®—å™¨"""
    
    @staticmethod
    def calculate_reserved_memory(
        reserved: float,
        mode: str,
        gpu_index: int,
        min_safe_reserve: float,
        gpu_manager: GPUManager
    ) -> Tuple[int, str]:
        """è®¡ç®—é¢„ç•™æ˜¾å­˜å¤§å°
        
        Args:
            reserved: ç”¨æˆ·è®¾ç½®çš„é¢„ç•™å€¼
            mode: æ¨¡å¼é€‰æ‹©
            gpu_index: GPUç´¢å¼•
            min_safe_reserve: æœ€å°å®‰å…¨ä¿ç•™æ˜¾å­˜
            gpu_manager: GPUç®¡ç†å™¨å®ä¾‹
            
        Returns:
            (é¢„ç•™æ˜¾å­˜å­—èŠ‚æ•°, è¯¦ç»†è¯´æ˜)
        """
        # è·å–GPUä¿¡æ¯
        memory_info = gpu_manager.get_gpu_memory_info(gpu_index)
        
        if mode == "manual":
            return MemoryStrategyCalculator._manual_mode(
                reserved, min_safe_reserve, memory_info
            )
        elif mode == "auto":
            return MemoryStrategyCalculator._auto_mode(
                reserved, min_safe_reserve, memory_info
            )
        elif mode == "smart":
            return MemoryStrategyCalculator._smart_mode(
                reserved, min_safe_reserve, memory_info
            )
        else:
            logger.warning(f"æœªçŸ¥æ¨¡å¼: {mode}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return int(max(reserved, min_safe_reserve) * GB_TO_BYTES), "é»˜è®¤æ¨¡å¼"
    
    @staticmethod
    def _manual_mode(
        reserved: float,
        min_safe_reserve: float,
        memory_info: Optional[Tuple[float, float, float]]
    ) -> Tuple[int, str]:
        """æ‰‹åŠ¨æ¨¡å¼"""
        # ç¡®ä¿ä¸ä½äºæœ€å°å®‰å…¨å€¼
        manual_reserved = max(reserved, min_safe_reserve)
        
        if memory_info:
            total_gb, used_gb, free_gb = memory_info
            max_allowed = total_gb * MAX_RESERVED_RATIO
            
            if manual_reserved > max_allowed:
                safe_reserved = max_allowed
                detail = f"æ‰‹åŠ¨æ¨¡å¼: {reserved:.2f}GB â†’ {safe_reserved:.2f}GB (é™åˆ¶ä¸ºæ€»æ˜¾å­˜çš„{MAX_RESERVED_RATIO*100:.0f}%)"
                logger.warning(detail)
                return int(safe_reserved * GB_TO_BYTES), detail
            
            detail = f"æ‰‹åŠ¨æ¨¡å¼: {manual_reserved:.2f}GB (æ€»{total_gb:.2f}GB, å·²ç”¨{used_gb:.2f}GB)"
            return int(manual_reserved * GB_TO_BYTES), detail
        else:
            detail = f"æ‰‹åŠ¨æ¨¡å¼: {manual_reserved:.2f}GB (æ— GPUä¿¡æ¯)"
            return int(manual_reserved * GB_TO_BYTES), detail
    
    @staticmethod
    def _auto_mode(
        reserved: float,
        min_safe_reserve: float,
        memory_info: Optional[Tuple[float, float, float]]
    ) -> Tuple[int, str]:
        """è‡ªåŠ¨æ¨¡å¼: å½“å‰ä½¿ç”¨é‡ + é¢„ç•™ç¼“å†²"""
        if memory_info is None:
            logger.warning("æ— æ³•è·å–GPUä¿¡æ¯ï¼Œè‡ªåŠ¨æ¨¡å¼å›é€€åˆ°æ‰‹åŠ¨æ¨¡å¼")
            return MemoryStrategyCalculator._manual_mode(reserved, min_safe_reserve, None)
        
        total_gb, used_gb, free_gb = memory_info
        
        # è®¡ç®—: å½“å‰ä½¿ç”¨ + ç¼“å†²
        auto_reserved = used_gb + reserved
        
        # ç¡®ä¿ä¸ä½äºæœ€å°å®‰å…¨å€¼
        auto_reserved = max(auto_reserved, min_safe_reserve)
        
        # ç¡®ä¿ä¸è¶…è¿‡æ€»æ˜¾å­˜çš„85%
        max_allowed = total_gb * 0.85
        safe_reserved = min(auto_reserved, max_allowed)
        
        if auto_reserved != safe_reserved:
            detail = f"è‡ªåŠ¨æ¨¡å¼: {auto_reserved:.2f}GB â†’ {safe_reserved:.2f}GB (å·²ç”¨{used_gb:.2f}GB + ç¼“å†²{reserved:.2f}GB, é™åˆ¶ä¸º85%)"
        else:
            detail = f"è‡ªåŠ¨æ¨¡å¼: {safe_reserved:.2f}GB (å·²ç”¨{used_gb:.2f}GB + ç¼“å†²{reserved:.2f}GB)"
        
        return int(safe_reserved * GB_TO_BYTES), detail
    
    @staticmethod
    def _smart_mode(
        reserved: float,
        min_safe_reserve: float,
        memory_info: Optional[Tuple[float, float, float]]
    ) -> Tuple[int, str]:
        """æ™ºèƒ½æ¨¡å¼: æ ¹æ®æ˜¾å­˜ä½¿ç”¨æƒ…å†µåŠ¨æ€è°ƒæ•´"""
        if memory_info is None:
            logger.warning("æ— æ³•è·å–GPUä¿¡æ¯ï¼Œæ™ºèƒ½æ¨¡å¼ä½¿ç”¨å®‰å…¨é»˜è®¤å€¼")
            default_value = max(reserved + 1.0, min_safe_reserve)
            return int(default_value * GB_TO_BYTES), f"æ™ºèƒ½æ¨¡å¼(æ— GPUä¿¡æ¯): {default_value:.2f}GB"
        
        total_gb, used_gb, free_gb = memory_info
        
        # è®¡ç®—å¯ç”¨æ˜¾å­˜æ¯”ä¾‹
        available_ratio = free_gb / total_gb
        
        # åŸºç¡€é¢„ç•™å€¼
        base_reserved = used_gb + reserved
        
        # æ ¹æ®å¯ç”¨æ˜¾å­˜æ¯”ä¾‹é€‰æ‹©ç­–ç•¥
        if available_ratio < 0.2:  # æ˜¾å­˜ç´§å¼ 
            strategy_ratio = MEMORY_STRATEGY["tight"]
            smart_reserved = max(base_reserved, total_gb * strategy_ratio)
            status = "ç´§å¼ "
        elif available_ratio < 0.4:  # æ˜¾å­˜ä¸­ç­‰
            strategy_ratio = MEMORY_STRATEGY["medium"]
            smart_reserved = base_reserved
            status = "ä¸­ç­‰"
        else:  # æ˜¾å­˜å……è¶³
            strategy_ratio = MEMORY_STRATEGY["loose"]
            smart_reserved = max(base_reserved, min_safe_reserve)
            status = "å……è¶³"
        
        # ç¡®ä¿ä¸ä½äºæœ€å°å®‰å…¨å€¼
        smart_reserved = max(smart_reserved, min_safe_reserve)
        
        # ç¡®ä¿ä¸è¶…è¿‡æ€»æ˜¾å­˜çš„90%
        max_allowed = total_gb * MAX_RESERVED_RATIO
        safe_reserved = min(smart_reserved, max_allowed)
        
        detail = (
            f"æ™ºèƒ½æ¨¡å¼: {safe_reserved:.2f}GB "
            f"(çŠ¶æ€:{status}, å¯ç”¨{free_gb:.2f}GB/{total_gb:.2f}GB={available_ratio*100:.1f}%)"
        )
        
        if smart_reserved != safe_reserved:
            detail += f" [è°ƒæ•´: {smart_reserved:.2f}â†’{safe_reserved:.2f}]"
        
        return int(safe_reserved * GB_TO_BYTES), detail

# ============================================================================
# ä¸»èŠ‚ç‚¹ç±»
# ============================================================================

class ReservedMemorySetter:
    """é¢„ç•™æ˜¾å­˜è®¾ç½®èŠ‚ç‚¹ - æ”¹è¿›ç‰ˆ"""
    
    @classmethod
    def INPUT_TYPES(cls) -> dict:
        return {
            "required": {
                "anything": (AlwaysEqualProxy("*"), {
                    "tooltip": "é€šç”¨è¾“å…¥ï¼Œç”¨äºè¿æ¥å·¥ä½œæµæ•°æ®"
                }),
                "reserved": ("FLOAT", {
                    "default": DEFAULT_RESERVED_GB,
                    "min": MIN_RESERVED_GB,
                    "max": 32.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "é¢„ç•™æ˜¾å­˜å¤§å°(GB)\nâ€¢ æ‰‹åŠ¨æ¨¡å¼: å›ºå®šé¢„ç•™\nâ€¢ è‡ªåŠ¨æ¨¡å¼: é¢å¤–ç¼“å†²\nâ€¢ æ™ºèƒ½æ¨¡å¼: åŠ¨æ€ä¼˜åŒ–"
                }),
                "mode": (["smart", "auto", "manual"], {
                    "default": "smart",
                    "tooltip": (
                        "æ¨¡å¼é€‰æ‹©:\n"
                        "â€¢ smart(æ¨è): æ ¹æ®æ˜¾å­˜çŠ¶æ€æ™ºèƒ½è°ƒæ•´\n"
                        "â€¢ auto: å½“å‰ä½¿ç”¨é‡ + é¢„ç•™ç¼“å†²\n"
                        "â€¢ manual: å›ºå®šé¢„ç•™å€¼"
                    )
                }),
                "gpu_index": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 7,
                    "step": 1,
                    "tooltip": "GPUè®¾å¤‡ç´¢å¼• (0-7)"
                }),
                "min_safe_reserve": ("FLOAT", {
                    "default": MIN_SAFE_RESERVE_GB,
                    "min": 0.5,
                    "max": 8.0,
                    "step": 0.5,
                    "display": "slider",
                    "tooltip": "æœ€å°å®‰å…¨ä¿ç•™æ˜¾å­˜(GB)ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®š"
                }),
                "clear_memory": ("BOOLEAN", {
                    "default": False,
                    "label_on": "âœ“ æ¸…ç†æ˜¾å­˜",
                    "label_off": "âœ— ä¸æ¸…ç†",
                    "tooltip": "æ‰§è¡Œå‰æ¸…ç†GPUæ˜¾å­˜ç¼“å­˜"
                }),
                "show_gpu_info": ("BOOLEAN", {
                    "default": True,
                    "label_on": "âœ“ æ˜¾ç¤ºä¿¡æ¯",
                    "label_off": "âœ— éšè—ä¿¡æ¯",
                    "tooltip": "æ˜¾ç¤ºè¯¦ç»†çš„GPUçŠ¶æ€ä¿¡æ¯"
                })
            },
            "hidden": {
                "unique_id": "UNIQUE_ID", 
                "extra_pnginfo": "EXTRA_PNGINFO"
            }
        }

    RETURN_TYPES = (AlwaysEqualProxy("*"),)
    RETURN_NAMES = ("output",)
    OUTPUT_NODE = True
    FUNCTION = "set_memory"
    CATEGORY = "ARES/æ˜¾å­˜ç®¡ç†"
    DESCRIPTION = "æ™ºèƒ½ç®¡ç†GPUæ˜¾å­˜é¢„ç•™ï¼Œæ”¯æŒä¸‰ç§æ¨¡å¼å’Œè‡ªåŠ¨æ¸…ç†"
    
    def __init__(self):
        self.gpu_manager = GPUManager()
        self.memory_cleaner = MemoryCleaner()
        self.calculator = MemoryStrategyCalculator()
    
    def set_memory(
        self, 
        anything: Any, 
        reserved: float, 
        mode: str = "smart",
        gpu_index: int = 0,
        min_safe_reserve: float = MIN_SAFE_RESERVE_GB,
        clear_memory: bool = False,
        show_gpu_info: bool = True,
        unique_id: Optional[str] = None, 
        extra_pnginfo: Optional[Any] = None
    ) -> Tuple[Any]:
        """è®¾ç½®é¢„ç•™æ˜¾å­˜
        
        Args:
            anything: é€šç”¨è¾“å…¥æ•°æ®
            reserved: é¢„ç•™æ˜¾å­˜å¤§å°(GB)
            mode: æ¨¡å¼é€‰æ‹© (smart/auto/manual)
            gpu_index: GPUè®¾å¤‡ç´¢å¼•
            min_safe_reserve: æœ€å°å®‰å…¨ä¿ç•™æ˜¾å­˜
            clear_memory: æ˜¯å¦æ¸…ç†æ˜¾å­˜
            show_gpu_info: æ˜¯å¦æ˜¾ç¤ºGPUä¿¡æ¯
            unique_id: èŠ‚ç‚¹å”¯ä¸€ID
            extra_pnginfo: é¢å¤–PNGä¿¡æ¯
            
        Returns:
            è¾“å…¥æ•°æ®çš„å…ƒç»„
        """
        try:
            # éªŒè¯GPUç´¢å¼•
            if not self.gpu_manager.validate_gpu_index(gpu_index):
                logger.error(f"GPUç´¢å¼• {gpu_index} æ— æ•ˆï¼Œä½¿ç”¨GPU 0")
                gpu_index = 0
            
            # æ˜¾ç¤ºGPUä¿¡æ¯
            if show_gpu_info:
                self._show_gpu_info(gpu_index)
            
            # æ¸…ç†æ˜¾å­˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if clear_memory:
                clean_result = self.memory_cleaner.clear_gpu_memory(gpu_index)
                if clean_result["success"] and clean_result["freed_memory_gb"] > 0:
                    logger.info(
                        f"âœ“ æ˜¾å­˜æ¸…ç†æˆåŠŸ: é‡Šæ”¾ {clean_result['freed_memory_gb']:.2f}GB "
                        f"({clean_result['freed_memory_mb']:.0f}MB)"
                    )
            
            # è®¡ç®—é¢„ç•™æ˜¾å­˜
            reserved_bytes, detail = self.calculator.calculate_reserved_memory(
                reserved, mode, gpu_index, min_safe_reserve, self.gpu_manager
            )
            
            # è®¾ç½®é¢„ç•™æ˜¾å­˜
            model_management.EXTRA_RESERVED_MEMORY = reserved_bytes
            
            # è¾“å‡ºè®¾ç½®ä¿¡æ¯
            reserved_gb = reserved_bytes / GB_TO_BYTES
            logger.info(f"âœ“ {detail}")
            logger.info(f"å·²è®¾ç½®é¢„ç•™æ˜¾å­˜: {reserved_gb:.2f}GB ({reserved_bytes / MB_TO_BYTES:.0f}MB)")
            
        except Exception as e:
            # å‡ºé”™æ—¶ä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼
            safe_default = max(DEFAULT_RESERVED_GB, min_safe_reserve)
            model_management.EXTRA_RESERVED_MEMORY = int(safe_default * GB_TO_BYTES)
            logger.error(f"è®¾ç½®é¢„ç•™æ˜¾å­˜æ—¶å‡ºé”™: {e}ï¼Œä½¿ç”¨å®‰å…¨é»˜è®¤å€¼ {safe_default:.2f}GB")

        return (anything,)
    
    def _show_gpu_info(self, gpu_index: int) -> None:
        """æ˜¾ç¤ºGPUè¯¦ç»†ä¿¡æ¯"""
        info = self.gpu_manager.get_detailed_info(gpu_index)
        
        if not info["available"]:
            logger.warning(f"GPU {gpu_index} ä¸å¯ç”¨")
            return
        
        # æ„å»ºä¿¡æ¯å­—ç¬¦ä¸²
        info_parts = [f"GPU {gpu_index}"]
        
        if info["name"]:
            info_parts.append(f"å‹å·: {info['name']}")
        
        if info["memory"]:
            total_gb, used_gb, free_gb = info["memory"]
            usage_percent = (used_gb / total_gb) * 100
            info_parts.append(
                f"æ˜¾å­˜: {used_gb:.2f}GB/{total_gb:.2f}GB "
                f"(ä½¿ç”¨ç‡{usage_percent:.1f}%, å¯ç”¨{free_gb:.2f}GB)"
            )
        
        if info["temperature"] is not None:
            info_parts.append(f"æ¸©åº¦: {info['temperature']}Â°C")
        
        if info["utilization"] is not None:
            info_parts.append(f"åˆ©ç”¨ç‡: {info['utilization']}%")
        
        logger.info(" | ".join(info_parts))

# ============================================================================
# æ˜¾å­˜ç›‘æ§èŠ‚ç‚¹ (é¢å¤–åŠŸèƒ½)
# ============================================================================

class GPUMemoryMonitor:
    """GPUæ˜¾å­˜ç›‘æ§èŠ‚ç‚¹ - ä»…ç”¨äºæŸ¥çœ‹ä¿¡æ¯ï¼Œä¸å½±å“å·¥ä½œæµ"""
    
    @classmethod
    def INPUT_TYPES(cls) -> dict:
        return {
            "required": {
                "gpu_index": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 7,
                    "step": 1,
                    "tooltip": "è¦ç›‘æ§çš„GPUç´¢å¼•"
                }),
                "refresh": ("BOOLEAN", {
                    "default": True,
                    "label_on": "âœ“ åˆ·æ–°",
                    "label_off": "âœ— æš‚åœ",
                    "tooltip": "æ˜¯å¦å®æ—¶åˆ·æ–°GPUä¿¡æ¯"
                })
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("gpu_info",)
    OUTPUT_NODE = True
    FUNCTION = "monitor"
    CATEGORY = "ARES/æ˜¾å­˜ç®¡ç†"
    DESCRIPTION = "å®æ—¶ç›‘æ§GPUæ˜¾å­˜å’ŒçŠ¶æ€ä¿¡æ¯"
    
    def __init__(self):
        self.gpu_manager = GPUManager()
    
    def monitor(self, gpu_index: int = 0, refresh: bool = True) -> Tuple[str]:
        """ç›‘æ§GPUçŠ¶æ€"""
        if not refresh:
            return ("ç›‘æ§å·²æš‚åœ",)
        
        info = self.gpu_manager.get_detailed_info(gpu_index)
        
        if not info["available"]:
            return (f"GPU {gpu_index} ä¸å¯ç”¨",)
        
        # æ„å»ºè¯¦ç»†ä¿¡æ¯
        lines = [
            f"=== GPU {gpu_index} çŠ¶æ€ ===",
            f"å‹å·: {info['name'] or 'æœªçŸ¥'}",
        ]
        
        if info["memory"]:
            total_gb, used_gb, free_gb = info["memory"]
            usage_percent = (used_gb / total_gb) * 100
            lines.extend([
                f"æ€»æ˜¾å­˜: {total_gb:.2f} GB",
                f"å·²ä½¿ç”¨: {used_gb:.2f} GB ({usage_percent:.1f}%)",
                f"å¯ç”¨: {free_gb:.2f} GB",
            ])
        
        if info["temperature"] is not None:
            lines.append(f"æ¸©åº¦: {info['temperature']}Â°C")
        
        if info["utilization"] is not None:
            lines.append(f"GPUåˆ©ç”¨ç‡: {info['utilization']}%")
        
        info_text = "\n".join(lines)
        logger.info(f"\n{info_text}")
        
        return (info_text,)

# ============================================================================
# æ‰¹é‡æ¸…ç†èŠ‚ç‚¹
# ============================================================================

class BatchMemoryCleaner:
    """æ‰¹é‡æ˜¾å­˜æ¸…ç†èŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls) -> dict:
        return {
            "required": {
                "anything": (AlwaysEqualProxy("*"), {
                    "tooltip": "é€šç”¨è¾“å…¥ï¼Œç”¨äºè¿æ¥å·¥ä½œæµ"
                }),
                "clear_all_gpus": ("BOOLEAN", {
                    "default": False,
                    "label_on": "âœ“ æ¸…ç†æ‰€æœ‰GPU",
                    "label_off": "âœ— ä»…å½“å‰GPU",
                    "tooltip": "æ˜¯å¦æ¸…ç†æ‰€æœ‰GPUçš„æ˜¾å­˜"
                }),
                "aggressive": ("BOOLEAN", {
                    "default": False,
                    "label_on": "âœ“ æ·±åº¦æ¸…ç†",
                    "label_off": "âœ— å¸¸è§„æ¸…ç†",
                    "tooltip": "æ·±åº¦æ¸…ç†æ¨¡å¼ä¼šé¢å¤–æ¸…ç†ComfyUIæ¨¡å‹ç¼“å­˜"
                })
            }
        }
    
    RETURN_TYPES = (AlwaysEqualProxy("*"), "STRING")
    RETURN_NAMES = ("output", "æ¸…ç†æŠ¥å‘Š")
    OUTPUT_NODE = True
    FUNCTION = "clean"
    CATEGORY = "ARES/æ˜¾å­˜ç®¡ç†"
    DESCRIPTION = "æ‰¹é‡æ¸…ç†GPUæ˜¾å­˜ç¼“å­˜"
    
    def __init__(self):
        self.memory_cleaner = MemoryCleaner()
    
    def clean(
        self,
        anything: Any,
        clear_all_gpus: bool = False,
        aggressive: bool = False
    ) -> Tuple[Any, str]:
        """æ‰§è¡Œæ¸…ç†æ“ä½œ"""
        report_lines = ["=== æ˜¾å­˜æ¸…ç†æŠ¥å‘Š ==="]
        
        try:
            if clear_all_gpus:
                # æ¸…ç†æ‰€æœ‰GPU
                gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
                total_freed = 0.0
                
                for i in range(gpu_count):
                    result = self.memory_cleaner.clear_gpu_memory(i)
                    if result["success"]:
                        freed = result["freed_memory_gb"]
                        total_freed += freed
                        report_lines.append(f"GPU {i}: é‡Šæ”¾ {freed:.2f}GB")
                
                report_lines.append(f"æ€»è®¡é‡Šæ”¾: {total_freed:.2f}GB")
            else:
                # ä»…æ¸…ç†å½“å‰GPU
                result = self.memory_cleaner.clear_gpu_memory(0)
                if result["success"]:
                    freed = result["freed_memory_gb"]
                    report_lines.append(f"é‡Šæ”¾æ˜¾å­˜: {freed:.2f}GB")
            
            # æ·±åº¦æ¸…ç†
            if aggressive:
                self.memory_cleaner.clear_all_caches()
                report_lines.append("å·²æ‰§è¡Œæ·±åº¦æ¸…ç†")
            
            report_lines.append("âœ“ æ¸…ç†å®Œæˆ")
            logger.info("\n".join(report_lines))
            
        except Exception as e:
            error_msg = f"æ¸…ç†å¤±è´¥: {e}"
            report_lines.append(f"âœ— {error_msg}")
            logger.error(error_msg)
        
        report = "\n".join(report_lines)
        return (anything, report)

# ============================================================================
# èŠ‚ç‚¹æ³¨å†Œ
# ============================================================================

NODE_CLASS_MAPPINGS = {
    "ReservedMemorySetter": ReservedMemorySetter,
    "GPUMemoryMonitor": GPUMemoryMonitor,
    "BatchMemoryCleaner": BatchMemoryCleaner
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "ReservedMemorySetter": "ğŸ›ï¸ æ™ºèƒ½æ˜¾å­˜é¢„ç•™",
    "GPUMemoryMonitor": "ğŸ“Š GPUæ˜¾å­˜ç›‘æ§",
    "BatchMemoryCleaner": "ğŸ§¹ æ‰¹é‡æ˜¾å­˜æ¸…ç†"
}

# ============================================================================
# æ¸…ç†å‡½æ•°
# ============================================================================

def cleanup():
    """ç¨‹åºé€€å‡ºæ—¶æ¸…ç†èµ„æº"""
    try:
        gpu_manager = GPUManager()
        gpu_manager.cleanup()
    except Exception as e:
        logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")

# æ³¨å†Œé€€å‡ºæ¸…ç†å‡½æ•°
atexit.register(cleanup)

# ============================================================================
# æ¨¡å—ä¿¡æ¯
# ============================================================================

__version__ = "2.0.0"
__author__ = "ARES"
__description__ = "ComfyUI GPUæ˜¾å­˜æ™ºèƒ½ç®¡ç†èŠ‚ç‚¹é›†"
